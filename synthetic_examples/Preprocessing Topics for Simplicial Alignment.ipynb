{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from graph_augmented_pt.utils.tensorboard_utils import *\n",
    "\n",
    "from graphlet_atlas import *\n",
    "from synthetic_datasets import *\n",
    "from synthetic_runner import *\n",
    "from simplicial_manfiolds import *\n",
    "\n",
    "import matplotlib, pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import copy, itertools, json, logging, math, os, pickle, scipy, shutil, time, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n) / f(r) / f(n-r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f4dafc060d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/crimea/conda_envs/graph_augmented_pt_2/lib/python3.8/site-packages/tqdm/std.py\", line 1121, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "RAW_FILEPATH = '/crimea/graph_augmented_pt/synthetic_datasets/wikisent2.txt'\n",
    "PKL_FILEPATH = '/crimea/graph_augmented_pt/synthetic_datasets/wikisent2_feat.pkl'\n",
    "SIMPLEX_ORDER_FILEPATH = '/crimea/graph_augmented_pt/synthetic_datasets/topics_to_try.pkl'\n",
    "TOPIC_CLIQUE_FILEPATH = '/crimea/graph_augmented_pt/synthetic_datasets/topic_clique.json'\n",
    "\n",
    "assert os.path.isfile(RAW_FILEPATH)\n",
    "assert os.path.isfile(PKL_FILEPATH)\n",
    "\n",
    "# Source: https://www.kaggle.com/mikeortman/wikipedia-sentences\n",
    "with open(RAW_FILEPATH, mode='r') as f: text_data = f.readlines()\n",
    "\n",
    "with open(PKL_FILEPATH, mode='rb') as f:\n",
    "    X, LDA, topics, first_topic, sents_by_topic, topic_correlations = pickle.load(f)\n",
    "n_sents_by_topic = {t: len(sents) for t, sents in sents_by_topic.items()}\n",
    "\n",
    "with open(SIMPLEX_ORDER_FILEPATH, mode='rb') as f:\n",
    "    topics_to_try = pickle.load(f)\n",
    "    \n",
    "with open(TOPIC_CLIQUE_FILEPATH, mode='r') as f:\n",
    "    topic_clique_hint = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A third different Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(tset, valid_simplices, base_ts=np.arange(100)):\n",
    "    tset = list(tset)\n",
    "\n",
    "    ts = []\n",
    "    for t1 in set(base_ts) - set(tset):\n",
    "        can_include = True\n",
    "        for t2, t3 in itertools.combinations(tset, 2):\n",
    "            if not frozenset((t1, t2, t3)) in valid_simplices:\n",
    "                can_include = False\n",
    "                break\n",
    "                \n",
    "        if can_include: ts.append(t1)\n",
    "        \n",
    "    return set(ts)\n",
    "        \n",
    "def maximally_expand(tset, valid_simplices, base_ts=np.arange(100), depth=0, memoization_dict=None):\n",
    "    if memoization_dict is None: memoization_dict = {}\n",
    "        \n",
    "    if frozenset(tset) in memoization_dict: return memoization_dict[frozenset(tset)]\n",
    "        \n",
    "    ts = expand(tset, valid_simplices, base_ts=base_ts)\n",
    "    if len(ts) == 0: return tset\n",
    "    \n",
    "    max_opt = tset\n",
    "    ts_rng = ts if (depth > 2 or len(ts) < 10) else tqdm(ts, leave=False, desc=\"Expanding\")\n",
    "    for t in ts_rng:\n",
    "        query_tset = frozenset([t, *tset])\n",
    "        new_tset = maximally_expand(\n",
    "            query_tset, valid_simplices, depth=depth+1, memoization_dict=memoization_dict\n",
    "        )\n",
    "        memoization_dict[query_tset] = new_tset\n",
    "        \n",
    "        if len(new_tset) > len(max_opt): max_opt = new_tset\n",
    "            \n",
    "    return max_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({0, 1, 2, 3, 4, 5})\n"
     ]
    }
   ],
   "source": [
    "fake_valid_simplices = set(\n",
    "    frozenset(s) for s in list(itertools.combinations(np.arange(6), 3)) + [(0, 1, 7), (7, 8, 9), (4, 8, 9)]\n",
    ")\n",
    "\n",
    "fake_valid_simplices_list = list(fake_valid_simplices)\n",
    "random.shuffle(fake_valid_simplices_list)\n",
    "\n",
    "o = []\n",
    "for s in fake_valid_simplices_list:\n",
    "    m = maximally_expand(s, fake_valid_simplices)\n",
    "    if len(m) > len(o): o = m\n",
    "        \n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2178743 sentences as they lack sufficient probability mass in their top-3.\n",
      "It took 0.0 minutes to do that check & drop.\n",
      "Observe 158547 simplices (of 161700 total possible) in total across 5693082 sentences.\n",
      "It took 0.0 minutes to get the top 3 topics / sent and 0.2 minutes to get the counts.\n",
      "After filtering out insufficiently dense simplices, we have 46965/4645898 simplices / sentences, respectively. This process took 0.2 minutes\n"
     ]
    }
   ],
   "source": [
    "topics_cp               = copy.deepcopy(topics)\n",
    "min_sents_per_simplex   = 25\n",
    "topics_thresh           = 2/3\n",
    "\n",
    "N = len(topics_cp)\n",
    "\n",
    "top_3_st = time.time()\n",
    "first_topics  = np.argmax(topics_cp, axis=1)\n",
    "topics_cp[np.arange(N), first_topics] = 0\n",
    "second_topics = np.argmax(topics_cp, axis=1)\n",
    "topics_cp[np.arange(N), second_topics] = 0\n",
    "third_topics = np.argmax(topics_cp, axis=1)\n",
    "top_3_end = time.time()\n",
    "\n",
    "reindex_st = time.time()\n",
    "top_3 = np.vstack((first_topics, second_topics, third_topics)).T\n",
    "top_3_probs = topics[np.arange(N), [first_topics, second_topics, third_topics]].T\n",
    "\n",
    "obs_probability_mass = top_3_probs.sum(axis=1)\n",
    "valid_sents_mask = (obs_probability_mass > topics_thresh)\n",
    "valid_sents_idx,  = np.where(valid_sents_mask)\n",
    "\n",
    "topics_cp   = topics_cp[valid_sents_mask]\n",
    "top_3       = top_3[valid_sents_mask]\n",
    "top_3_probs = top_3_probs[valid_sents_mask]\n",
    "reindex_end = time.time()\n",
    "\n",
    "print(\n",
    "    f\"Dropping {len(topics) - len(top_3)} sentences as they lack sufficient probability mass in their top-3.\\n\"\n",
    "    f\"It took {(reindex_end - reindex_st)/60:.1f} minutes to do that check & drop.\"\n",
    ")\n",
    "\n",
    "cnt_assignments_st = time.time()\n",
    "all_observed_topic_simplices = Counter(frozenset(t) for t in top_3)\n",
    "cnt_assignments_end = time.time()\n",
    "\n",
    "print(\n",
    "    f\"Observe {len(all_observed_topic_simplices)} simplices (of {nCr(100, 3)} total possible) \"\n",
    "    f\"in total across {len(topics_cp)} sentences.\\n\"\n",
    "    f\"It took {(top_3_end - top_3_st)/60:.1f} minutes to get the top 3 topics / sent and \"\n",
    "    f\"{(cnt_assignments_end - cnt_assignments_st)/60:.1f} minutes to get the counts.\"\n",
    ")\n",
    "\n",
    "first_filtering_st      = time.time()\n",
    "valid_simplices         = set(k for k, v in all_observed_topic_simplices.items() if v >= min_sents_per_simplex)\n",
    "valid_simplex_checker   = lambda np_arr: np.array([frozenset(row) in valid_simplices for row in np_arr])\n",
    "sufficiently_dense_mask = valid_simplex_checker(top_3)\n",
    "\n",
    "topics_cp               = topics_cp[sufficiently_dense_mask]\n",
    "top_3                   = top_3[sufficiently_dense_mask]\n",
    "top_3_probs             = top_3_probs[sufficiently_dense_mask]\n",
    "valid_sents_idx         = valid_sents_idx[sufficiently_dense_mask]\n",
    "first_filtering_end     = time.time()\n",
    "\n",
    "print(\n",
    "    f\"After filtering out insufficiently dense simplices, we have {len(valid_simplices)}/{len(topics_cp)} \"\n",
    "    f\"simplices / sentences, respectively. This process took \"\n",
    "    f\"{(first_filtering_end - first_filtering_st)/60:.1f} minutes\"\n",
    ")\n",
    "\n",
    "global_selection = []\n",
    "containing_maximal_cliques = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_complete_subgraph_st = time.time() \n",
    "valid_simplices_list = list(valid_simplices)\n",
    "random.shuffle(valid_simplices_list)\n",
    "\n",
    "valid_simplices_rng = tqdm(\n",
    "    np.arange(len(valid_simplices_list)), desc=\"Complete Subgraph: 3 so far\"\n",
    ")\n",
    "for tset_idx in valid_simplices_rng:\n",
    "    tset = valid_simplices_list[tset_idx]\n",
    "    \n",
    "    max_expansion = maximally_expand(\n",
    "        tset, valid_simplices, memoization_dict=containing_maximal_cliques, \n",
    "    )\n",
    "    \n",
    "    if len(max_expansion) <= len(global_selection): continue\n",
    "        \n",
    "    global_selection = max_expansion\n",
    "    valid_simplices_rng.set_description(f\"Complete Subgraph: {len(global_selection)} so far\")\n",
    "        \n",
    "    # Given we found a new optimum, we want to take advantage of that.\n",
    "    \n",
    "    fresh_optima = True\n",
    "    while fresh_optima:\n",
    "        fresh_optima = False\n",
    "        for subclique_size in range(len(global_selection)-1, 2, -1):\n",
    "            for tset in itertools.combinations(global_selection, subclique_size):\n",
    "                m = maximally_expand(tset, valid_simplices, memoization_dict=containing_maximal_cliques)\n",
    "                if len(m) > len(global_selection):\n",
    "                    global_selection = m\n",
    "                    fresh_optima = True\n",
    "        \n",
    "valid_topics = global_selection\n",
    "valid_simplices = {t_set for t_set in valid_simplices if t_set.issubset(valid_topics)}\n",
    "assert len(valid_simplices) == nCr(len(valid_topics), 3)\n",
    "    \n",
    "simplex_valid_mask = np.array([frozenset(row) in valid_simplices for row in top_3])\n",
    "\n",
    "topics_cp                = topics_cp[simplex_valid_mask]\n",
    "top_3                    = top_3[simplex_valid_mask]\n",
    "top_3_probs              = top_3_probs[simplex_valid_mask]\n",
    "valid_sents_idx          = valid_sents_idx[simplex_valid_mask]\n",
    "to_complete_subgraph_end = time.time()\n",
    "\n",
    "print(\n",
    "    \"After filtering out simplices that are not universally compatible, we have \"\n",
    "    f\"{len(valid_simplices)}/{len(topics_cp)} \"\n",
    "    f\"simplices / sentences, respectively. This process took \"\n",
    "    f\"{(to_complete_subgraph_end - to_complete_subgraph_st)/60:.1f} minutes\"\n",
    ")\n",
    "\n",
    "\n",
    "normalization_st = time.time()\n",
    "normalized = np.divide(top_3_probs, top_3_probs.sum(axis=1)[:, np.newaxis])\n",
    "entropy    = -(normalized * np.log(normalized)).sum(axis=1)\n",
    "entropy_per_simplex = defaultdict(list)\n",
    "for i, (e, ts) in enumerate(zip(entropy, top_3)): entropy_per_simplex[frozenset(ts)].append(e)\n",
    "    \n",
    "agg_entropy_per_simplex = {\n",
    "    k: (np.min(es), np.max(es), np.histogram(es)) for k, es in entropy_per_simplex.items()\n",
    "}\n",
    "normalization_end = time.time()\n",
    "\n",
    "print(f\"Normalizing & computing entropy took {(normalization_end - normalization_st)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOPIC_CLIQUE_FILEPATH, mode='w') as f:\n",
    "    f.write(\n",
    "        json.dumps([int(t) for t in global_selection]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/crimea/graph_augmented_pt/synthetic_datasets/topic_containing_maximal_cliques.pkl', mode='wb') as f:\n",
    "    pickle.dump(containing_maximal_cliques, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
